import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time

mlb_teams = [
    "ATL", "ARI", "BAL", "BOS", "CHC", "CHW", "CIN", "CLE", "COL", "DET",
    "HOU", "KCR", "LAA", "LAD", "MIA", "MIL", "MIN", "NYM", "NYY", "OAK",
    "PHI", "PIT", "SDP", "SEA", "SFG", "STL", "TBR", "TEX", "TOR", "WSN"
]

def parse_team_schedule(team_abbr, year):
    url = f"https://www.baseball-reference.com/teams/{team_abbr}/{year}-schedule-scores.shtml"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.find("table", {"id": "team_schedule"})
    if not table or not table.tbody:
        return []

    rows = table.tbody.find_all("tr")
    games = []
    for row in rows:
        if 'class' in row.attrs and 'thead' in row['class']:
            continue
        cells = row.find_all("td")
        if not cells:
            continue
        try:
            date_str = row.find("th", {"data-stat": "date_game"}).text.strip()
            if "," in date_str:
                date = datetime.strptime(date_str, "%A, %b %d, %Y").date()
            else:
                date = datetime.strptime(date_str, "%A, %b %d").replace(year=year).date()
            opponent = row.find("td", {"data-stat": "opp_ID"}).text.strip()
            result = row.find("td", {"data-stat": "win_loss_result"}).text.strip()
            runs_scored = row.find("td", {"data-stat": "R"}).text.strip()
            runs_allowed = row.find("td", {"data-stat": "RA"}).text.strip()
            location = "home" if row.find("td", {"data-stat": "homeORroad"}).text.strip() != "@" else "away"
            if not (runs_scored and runs_allowed and opponent and result):
                continue
            games.append({
                "date": date,
                "team": team_abbr,
                "opponent": opponent,
                "location": location,
                "team_score": int(runs_scored),
                "opponent_score": int(runs_allowed)
            })
        except:
            continue
    return games

def scrape_mlb_schedule(year=2024, delay=1.0):
    all_games = []
    for team in mlb_teams:
        print(f"Scraping {team}...")
        all_games.extend(parse_team_schedule(team, year))
        time.sleep(delay)
    df = pd.DataFrame(all_games)
    df.drop_duplicates(inplace=True)
    df.to_csv(f"mlb_games_{year}.csv", index=False)
    print(f"Saved {len(df)} games to mlb_games_{year}.csv")

if __name__ == "__main__":
    scrape_mlb_schedule(2024)
